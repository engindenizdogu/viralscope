{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6500c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import io\n",
    "import json\n",
    "import zstandard as zstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56228703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "# 1. Define your ORIGINAL large source files\n",
    "path_to_folder = '../RawData/'\n",
    "file_bridge    = '_raw_yt_metadata.jsonl.zst'   # The Master/Anchor file (JSONL Zstandard)\n",
    "file_channels  = '_raw_df_channels.tsv.gz'     # Left Side\n",
    "file_time      = '_raw_df_timeseries.tsv.gz'   # Left Side\n",
    "file_num_comm  = 'num_comments.tsv.gz'         # Right Side\n",
    "file_yt_comm   = 'youtube_comments.tsv.gz'     # Right Side\n",
    "\n",
    "# 2. Output settings\n",
    "target_sample_size = 100_000\n",
    "out_prefix = '../SampleData/anchor_'\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dc94da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Anchoring on ../RawData/_raw_yt_metadata.jsonl.zst...\n",
      "   -> Streaming and sampling ~10% of each chunk into pool...\n",
      "   -> Candidate pool size: 420,000\n",
      "   -> Candidate pool size: 420,000\n",
      "   -> Saved anchor file with 100000 rows.\n",
      "   -> Keys loaded: 6857 Channels, 99995 Videos.\n",
      "   -> Saved anchor file with 100000 rows.\n",
      "   -> Keys loaded: 6857 Channels, 99995 Videos.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# STEP 1: CREATE THE ANCHOR (THE BRIDGE)\n",
    "# ==========================================\n",
    "print(f\"1. Anchoring on {path_to_folder + file_bridge}...\")\n",
    "\n",
    "# For JSONL.ZST: stream-decompress and parse line-by-line, collecting a candidate pool\n",
    "bridge_rows = []  # list of dicts\n",
    "\n",
    "# Stream read the ZST JSONL; build a candidate pool by fractional sampling per chunk\n",
    "# We'll read in manageable text buffers; within each buffer, sample a fraction to keep memory bounded.\n",
    "pool_target_multiplier = 4  # stop when we have ~4x target in the pool\n",
    "fraction_per_chunk = 0.1    # similar strategy to previous chunk sampling\n",
    "\n",
    "# Helper to sample fraction from a list without converting to DataFrame yet\n",
    "def sample_fraction(records, frac, rng=random.Random(42)):\n",
    "    if not records:\n",
    "        return []\n",
    "    # Convert fraction to count\n",
    "    k = max(1, int(len(records) * frac))\n",
    "    # If k >= len(records), just return the records\n",
    "    if k >= len(records):\n",
    "        return records\n",
    "    return rng.sample(records, k)\n",
    "\n",
    "print(f\"   -> Streaming and sampling ~{int(fraction_per_chunk*100)}% of each chunk into pool...\")\n",
    "\n",
    "# Read the .jsonl.zst file\n",
    "chunk_lines_target = target_sample_size * 2  # logical chunk size for pooling\n",
    "current_pool_size = 0\n",
    "rng = random.Random(42)\n",
    "\n",
    "with open(path_to_folder + file_bridge, 'rb') as fh:\n",
    "    dctx = zstd.ZstdDecompressor()\n",
    "    with dctx.stream_reader(fh) as reader:\n",
    "        text_stream = io.TextIOWrapper(reader, encoding='utf-8')\n",
    "        chunk_buffer = []\n",
    "        for i, line in enumerate(text_stream):\n",
    "            try:\n",
    "                rec = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue  # skip malformed lines safely\n",
    "            chunk_buffer.append(rec)\n",
    "\n",
    "            # When buffer reaches target, sample a fraction and add to pool\n",
    "            if len(chunk_buffer) >= chunk_lines_target:\n",
    "                sampled = sample_fraction(chunk_buffer, fraction_per_chunk, rng)\n",
    "                bridge_rows.extend(sampled)\n",
    "                current_pool_size = len(bridge_rows)\n",
    "                # Clear buffer\n",
    "                chunk_buffer.clear()\n",
    "                # Safety stop when pool large enough\n",
    "                if current_pool_size > target_sample_size * pool_target_multiplier:\n",
    "                    break\n",
    "        # Handle any remaining buffer\n",
    "        if chunk_buffer:\n",
    "            sampled = sample_fraction(chunk_buffer, fraction_per_chunk, rng)\n",
    "            bridge_rows.extend(sampled)\n",
    "\n",
    "print(f\"   -> Candidate pool size: {len(bridge_rows):,}\")\n",
    "\n",
    "# Combine candidates and take the exact 'target_sample_size'\n",
    "# Convert to DataFrame for downstream processing\n",
    "if not bridge_rows:\n",
    "    raise RuntimeError(\"No records were collected from the bridge file. Check input path/format.\")\n",
    "\n",
    "df_bridge_pool = pd.DataFrame(bridge_rows)\n",
    "if len(df_bridge_pool) < target_sample_size:\n",
    "    raise RuntimeError(f\"Pool smaller than target_sample_size: {len(df_bridge_pool)} < {target_sample_size}\")\n",
    "\n",
    "df_final_bridge = df_bridge_pool.sample(n=target_sample_size, random_state=42)\n",
    "\n",
    "# Save the Master Bridge File as TSV.GZ to match downstream reading\n",
    "# Ensure required keys exist\n",
    "required_cols = []\n",
    "if 'channel_id' in df_final_bridge.columns:\n",
    "    required_cols.append('channel_id')\n",
    "else:\n",
    "    raise KeyError(\"'channel_id' not found in bridge records.\")\n",
    "\n",
    "# display_id in yt metadata maps to video_id on some files; verify presence\n",
    "if 'display_id' in df_final_bridge.columns:\n",
    "    required_cols.append('display_id')\n",
    "else:\n",
    "    # Sometimes the field might be named 'video_id' directly\n",
    "    if 'video_id' in df_final_bridge.columns:\n",
    "        df_final_bridge['display_id'] = df_final_bridge['video_id']\n",
    "        required_cols.append('display_id')\n",
    "    else:\n",
    "        raise KeyError(\"Neither 'display_id' nor 'video_id' found in bridge records.\")\n",
    "\n",
    "# Persist full sampled bridge, not just required columns\n",
    "output_path = (out_prefix + file_bridge.replace('.jsonl.zst', '.csv')).replace('__', '_')\n",
    "df_final_bridge.to_csv(output_path, index=False)\n",
    "print(f\"   -> Saved anchor file with {len(df_final_bridge)} rows.\")\n",
    "\n",
    "# Extract the keys we need to filter other files\n",
    "valid_channels = set(df_final_bridge['channel_id'].unique())\n",
    "valid_videos   = set(df_final_bridge['display_id'].unique()) # 'display_id' maps to 'video_id' later\n",
    "\n",
    "print(f\"   -> Keys loaded: {len(valid_channels)} Channels, {len(valid_videos)} Videos.\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 2: FILTER LEFT SIDE (Channel Based)\n",
    "# ==========================================\n",
    "# Maps filename to the column name inside it\n",
    "left_files_map = {\n",
    "    file_channels: 'channel',\n",
    "    file_time:     'channel'\n",
    "}\n",
    "\n",
    "for fname, col_name in left_files_map.items():\n",
    "    print(f\"2. Processing {path_to_folder + fname}...\")\n",
    "    \n",
    "    out_name = f\"{out_prefix + fname}\"\n",
    "    first_chunk = True\n",
    "    \n",
    "    with pd.read_csv(path_to_folder + fname, sep='\\t', compression='gzip', chunksize=50000) as reader:\n",
    "        for chunk in reader:\n",
    "            # Keep row ONLY if the channel existed in our Bridge sample\n",
    "            if col_name not in chunk.columns:\n",
    "                continue\n",
    "            mask = chunk[col_name].isin(valid_channels)\n",
    "            filtered_chunk = chunk[mask]\n",
    "            \n",
    "            if not filtered_chunk.empty:\n",
    "                mode = 'w' if first_chunk else 'a'\n",
    "                header = first_chunk\n",
    "                filtered_chunk.to_csv(out_name.replace('.tsv.gz', '.csv').replace('__', '_'), index=False, mode=mode, header=header)\n",
    "                first_chunk = False\n",
    "\n",
    "# ==========================================\n",
    "# STEP 3: FILTER RIGHT SIDE (Video Based)\n",
    "# ==========================================\n",
    "right_files_map = {\n",
    "    file_num_comm: 'display_id',\n",
    "    file_yt_comm:  'video_id'    # WATCH OUT: This file uses 'video_id'\n",
    "}\n",
    "\n",
    "for fname, col_name in right_files_map.items():\n",
    "    print(f\"3. Processing {path_to_folder + fname}...\")\n",
    "    \n",
    "    out_name = f\"{out_prefix + fname}\"\n",
    "    first_chunk = True\n",
    "    \n",
    "    with pd.read_csv(path_to_folder + fname, sep='\\t', compression='gzip', chunksize=50000) as reader:\n",
    "        for chunk in reader:\n",
    "            # Keep row ONLY if the video ID existed in our Bridge sample\n",
    "            if col_name not in chunk.columns:\n",
    "                continue\n",
    "            if col_name == 'video_id':\n",
    "                mask = chunk[col_name].isin(valid_videos)\n",
    "            else:  # 'display_id' case\n",
    "                mask = chunk[col_name].isin(valid_videos)\n",
    "            filtered_chunk = chunk[mask]\n",
    "            \n",
    "            if not filtered_chunk.empty:\n",
    "                mode = 'w' if first_chunk else 'a'\n",
    "                header = first_chunk\n",
    "                filtered_chunk.to_csv(out_name.replace('.tsv.gz', '.csv').replace('__', '_'), index=False, mode=mode, header=header)\n",
    "                first_chunk = False\n",
    "\n",
    "# ==========================================\n",
    "end_time = time.time()\n",
    "print(\"=\"*30)\n",
    "print(\"ANCHOR SAMPLING COMPLETE!\")\n",
    "print(f\"Total processing time: {end_time - start_time:.2f} seconds.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stevens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
